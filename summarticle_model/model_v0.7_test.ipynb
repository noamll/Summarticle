{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # only uncomment if not already downloaded\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6303f79-458a-46b0-94af-820a3a4d65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleToSummarize = \"pdf_database\\pdf3.pdf\"\n",
    "\n",
    "def pdfTE(pdfFile, version=1, start=0, end=0): # Function for text extraction from PDF. # Version states which version to use, with arguments(start, end) being the starting page and ending page.\n",
    "    if version==1:\n",
    "        with open(pdfFile, \"rb\") as file: # Read in binary to handle breakline statements better (\\n)\n",
    "            pdfReader = PdfReader(file)\n",
    "            for page in pdfReader.pages:\n",
    "                yield page.extract_text() # Use of generator as keeping the whole article in the memory results in memory error.\n",
    "    else:\n",
    "        with open(pdfFile, \"rb\") as file: # Read in binary to handle breakline statements better (\\n)\n",
    "            pdfReader = PdfReader(file)\n",
    "            if end==0:\n",
    "                end=len(pdfReader.pages)-1\n",
    "            for num in range(start,end): # Iterator over page (num) \n",
    "                yield pdfReader.pages[num].extract_text() # Text extraction of page (num)\n",
    "            \n",
    "def articleC(pdfFile, version=1,start=0,end=0): # Function to return the article in one string # Version and start, end arguments necessary for pdfTE integration.\n",
    "    if version==1:\n",
    "        textCombiner = pdfTE(pdfFile)\n",
    "    else:\n",
    "        textCombiner = pdfTE(pdfFile,2,start,end) # Choosing for version 2 if sumArticle2 failed on version 1\n",
    "    textCombined = \"\"\n",
    "    for text in textCombiner: # Loop over generator object to sum text of pages into one string\n",
    "        textCombined += text\n",
    "    if textCombined == \"\":\n",
    "        raise Exception(\"Oops! The task failed as the PDF file is empty.\") # Exception that handles empty files\n",
    "    elif len(textCombined) < 10:\n",
    "        raise Exception(\"Oops! The task failed as the PDF file has too little characters.\") # Exception that handles files with too little characters for a summary\n",
    "    else:\n",
    "        return textCombined\n",
    "\n",
    "def sumArticle2(pdfFile): # Function to summarize article as a whole\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\") # Model used from the huggingface hub (https://huggingface.co/pszemraj/led-large-book-summary)\n",
    "            articleCombined = articleC(pdfFile) # Iterate over text combiner function\n",
    "            lengthArticle = len(articleCombined)\n",
    "            summarizedPage = summarizer(articleCombined,max_length=1000, min_length=200, do_sample=True) # Max/min length for length of summarization in characters.\n",
    "            return summarizedPage[0][\"summary_text\"] # Return summary as string\n",
    "        except PdfReadError as PRE: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {PRE} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as OS: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {OS} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except (RuntimeError, IndexError) as RIE: # Exception check if pdf fails to be summarized due to it having too many tokens (> 16384)\n",
    "            print(f\"Oops! {RIE}, The file was too big to configure.\")\n",
    "            try:\n",
    "                print(\"Warning, this summary may take a while to produce.\\n\") # Give feedback to the user that because of the large pdf, the models computation will be larger\n",
    "                start = 0\n",
    "                summary = \"\" # Empty string to merge all the separate summaries in\n",
    "                length = len(PdfReader(ArticleToSummarize).pages) # Amount of pages in article\n",
    "                amountPages = (length//(lengthArticle//65536+1))+1 # Optimalization to determine best end to achieve least amount of computation (65536 amount of chars model can handle)\n",
    "                print(amountPages)\n",
    "                end = amountPages\n",
    "                lastRun = False\n",
    "                while True: # Loop till all the pages have been summarized\n",
    "                    print(f\"Beginning is {start}, {end}\")\n",
    "                    articleCombined = articleC(pdfFile,2,start,end)\n",
    "                    print(\"ArticleCombined done\")\n",
    "                    summarizedPage = summarizer(articleCombined,max_length=1000, min_length=200, do_sample=False) # Max/min length for length of summarization  per amount of pages in characters.\n",
    "                    print(\"SummarizedPage done\")\n",
    "                    summary += f\"{summarizedPage[0]['summary_text']} \"\n",
    "                    print(\"Summary added\")\n",
    "                    start += amountPages # Calculations to determine next start/end point of iteration.\n",
    "                    print(f\"Start is {start}\")\n",
    "                    end += amountPages\n",
    "                    print(f\"End is {end}\")\n",
    "                    print(\"Incrementer done\")\n",
    "                    if lastRun or start>=length: # Checker if program needs to stop executing\n",
    "                        print(\"Start>=length check done\")\n",
    "                        break\n",
    "                    if end>length or end==length: # Once summarizer reaches last page in summarization, it tells program to stop after one last run\n",
    "                        end=length-1\n",
    "                        print(\"End>length check done\")\n",
    "                        lastRun = True\n",
    "                        print(\"lastRun set to True\")\n",
    "                print(\"While loop done\")        \n",
    "                summaryF = summarizer(summary,max_length=2000, min_length=200, do_sample=False)        \n",
    "                return summaryF[0]['summary_text']\n",
    "            except Exception as E: # Exception that handles all different errors and asks user to feedback the error to the devs to enhance the model.\n",
    "                return f\"Oops! An unexpected error occured, {E}. Please report the error to the team.\"\n",
    "    else:\n",
    "        raise Exception(\"The file format is not a valid pdf.\")\n",
    "\n",
    "def pdfKE(pdfFile, language='english'): # Function to extract keywords from the PDF, language input needed as language needs to be part of stopwords folder. Standard keyword is english\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            articleCombined = articleC(pdfFile).lower() # Iterate over generator\n",
    "            tokens = word_tokenize(articleCombined) # Tokenize all words in the article\n",
    "            punctuations = [\"(\",\")\",\";\",\":\",\"[\",\"]\",\",\",\"!\",\"=\",\"==\",\"<\",\">\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\".\",\"//\",\"{\",\"}\",\"...\",\"``\",\"+\",\"\\'\\'\",\"-\",\"~\",\"\\\"\",\"â€™\",]\n",
    "            stopWords = stopwords.words(f'{language}')\n",
    "            keywords = [word for word in tokens if word not in stopWords and word not in punctuations] # Filter the words so that mostlikely keywords will be extracted\n",
    "            keywordExtracted = pd.Series(keywords).value_counts().index[:5] # Keywords formatting as a list\n",
    "            keywordDict = {i+1:keywordExtracted[i] for i in range(5)}\n",
    "            return keywordDict\n",
    "        except PdfReadError as PRE: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {PRE} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as OS: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {OS} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except Exception as E: # Exception that handles all different errors and asks user to feedback the error to the devs to enhance the model.\n",
    "            return f\"Oops! An unexpected error occured, {E}. Please report the error to the team.\"\n",
    "    else:\n",
    "        raise Exception(\"The file format is not a valid pdf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28510 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops! index out of range in self, The file was too big to configure.\n",
      "Warning, this summary may take a while to produce.\n",
      "\n",
      "17\n",
      "Beginning is 0, 17\n",
      "ArticleCombined done\n",
      "SummarizedPage done\n",
      "Summary added\n",
      "Start is 17\n",
      "End is 34\n",
      "Incrementer done\n",
      "End>length check done\n",
      "Beginning is 17, 31\n",
      "ArticleCombined done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 2000, but your input_length is only 436. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=218)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummarizedPage done\n",
      "Summary added\n",
      "Start is 34\n",
      "End is 48\n",
      "Incrementer done\n",
      "Start>=length check done\n",
      "While loop done\n",
      "This paper describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley is an autonomous, machine-learning robot designed to explore unre-heared, off-road terrain. It uses state-of-the-art artificial intelligence to predict future obstacles and make decisions quickly. The distinguishing feature of this study is the use of multiple machine learning algorithms to develop a prediction engine that outperforms previous estimates by more than a factor of ten. In both the 2004 and 2005 races, Stanley outperforms all other vehicles in terms of speed and accuracy. For the 2005 race, the safety of the vehicle poses a problem, so the team uses a combination of traditional radar, chemical/metabolic acid sensors, and several different types of sensing. These sensing devices are coupled with a \"learning algorithm\" that uses statistical methods such as averaging and batching. The goal of this approach is to predict when the vehicle will be closest to the object it is trying to reach. Stanley proves to be very accurate at times, but his overall accuracy is rather low. Using a velocity controller that constantly estimates terrain slope and ruggedness to predict the appropriate speed, Stan-ley's path planner determines the most appropriate \"velocity\" for each stage of the race. The final product is a simulation of the winning robot, Stanley; there are some hiccups, such as stalling of the laser data stream due to an incorrect time stamp; but overall, the results are very promising.\n"
     ]
    }
   ],
   "source": [
    "# Summarize text\n",
    "text = sumArticle2(ArticleToSummarize)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'stanley', 2: 'vehicle', 3: 'figure', 4: '/h20850', 5: 'online'}\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords and output them in dictionary format\n",
    "keywords = pdfKE(ArticleToSummarize)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dit document beschrijft de robot Stanley, die de 2005 DARPA Grand Challenge won. Stanley is een autonome, machine-learning robot ontworpen om onherhorende, off-road terrein te verkennen. Het maakt gebruik van state-of-the-art kunstmatige intelligentie om toekomstige obstakels te voorspellen en snel beslissingen te nemen. Het onderscheidende kenmerk van deze studie is het gebruik van meerdere machine learning algoritmen om een voorspelling motor te ontwikkelen die eerder schattingen overtreft met meer dan een factor tien. In zowel de 2004 en 2005 races, Stanley overtreft alle andere voertuigen in termen van snelheid en nauwkeurigheid. Voor de 2005 race, de veiligheid van het voertuig vormt een probleem, dus het team maakt gebruik van een combinatie van traditionele radar, chemische / metabolische zure sensoren, en verschillende soorten sensors. Deze sensoren zijn gekoppeld aan een \"learning algoritme\" dat gebruik maakt van statistische methoden zoals gemiddelde en batching. Het doel van deze aanpak is om te voorspellen wanneer het voertuig het dichtst bij het object is. Stanley blijkt zeer nauwkeurig te zijn.\n"
     ]
    }
   ],
   "source": [
    "# Summarization translation\n",
    "translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-nl\") # Model used for the translation, imported from Huggingface (https://huggingface.co/Helsinki-NLP/opus-mt-en-nl)\n",
    "translatedText = translation(text)[0]['translation_text']\n",
    "print(translatedText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
