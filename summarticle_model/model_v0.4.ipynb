{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # only uncomment if not already downloaded\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6303f79-458a-46b0-94af-820a3a4d65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleToSummarize = \"pdf_database\\pdf1.pdf\"\n",
    "\n",
    "def pdfTE(pdfFile): # Function for text extraction from PDF.\n",
    "    with open(pdfFile, \"rb\") as file: # Read in binary to handle breakline statements better (\\n)\n",
    "        pdfReader = PdfReader(file)\n",
    "        for page in pdfReader.pages:\n",
    "            yield page.extract_text() # Use of generator as keeping the whole article in the memory results in memory error.\n",
    "            \n",
    "def articleC(pdfFile): # Function to return the article in one string\n",
    "    textCombiner = pdfTE(pdfFile) \n",
    "    textCombined = \"\"\n",
    "    for text in textCombiner: # Loop over generator object to sum text of pages into one string\n",
    "        textCombined += text\n",
    "    if textCombined == \"\":\n",
    "        raise Exception(\"Oops! The task failed as the PDF file is empty.\")\n",
    "    elif len(textCombined) < 10:\n",
    "        raise Exception(\"Oops! The task failed as the PDF file has too little characters.\")\n",
    "    else:\n",
    "        return textCombined\n",
    "\n",
    "def sumArticle1(pdfFile): # Function to summarize article per page\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\") # Model used from the huggingface hub (https://huggingface.co/pszemraj/led-large-book-summary)\n",
    "            for text in pdfTE(pdfFile): # Iterate over generator\n",
    "                summarizedPage = summarizer(text,max_length=100, min_length=50, do_sample=True)\n",
    "                print(summarizedPage, end=\"\\n\")\n",
    "                print()\n",
    "        except PdfReadError as pre: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {pre} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as os: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {os} happened. Please retry the task once the issue has been resolved.\"\n",
    "    else:\n",
    "        return f\"Oops! The task failed. Please retry the task once the issue has been resolved.\" # Message if program fails to execute\n",
    "\n",
    "def sumArticle2(pdfFile): # Function to summarize article as a whole\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\") # Model used from the huggingface hub (https://huggingface.co/pszemraj/led-large-book-summary)\n",
    "            articleCombined = articleC(pdfFile) # Iterate over generator\n",
    "            summarizedPage = summarizer(articleCombined,max_length=1000, min_length=200, do_sample=False) # Max/min length for length of summarization in characters.\n",
    "            return summarizedPage[0][\"summary_text\"]\n",
    "        except PdfReadError as pre: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {pre} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as os: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {os} happened. Please retry the task once the issue has been resolved.\"\n",
    "    else:\n",
    "        return f\"Oops! The task failed. Please retry the task once the issue has been resolved.\" # Message if program fails to execute\n",
    "\n",
    "def pdfKE(pdfFile, language): # Function to extract keywords from the PDF, language input needed as language needs to be part of stopwords folder.\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            articleCombined = articleC(pdfFile).lower() # Iterate over generator\n",
    "            tokens = word_tokenize(articleCombined) # Tokenize all words in the article\n",
    "            punctuations = [\"(\",\")\",\";\",\":\",\"[\",\"]\",\",\",\"!\",\"=\",\"==\",\"<\",\">\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\".\",\"//\",\"{\",\"}\",\"...\",\"``\",\"+\",\"\\'\\'\",\"-\",\"~\",\"\\\"\",\"â€™\",]\n",
    "            stopWords = stopwords.words(f'{language}')\n",
    "            keywords = [word for word in tokens if word not in stopWords and word not in punctuations] # Filter the words so that mostlikely keywords will be extracted\n",
    "            keywordExtracted = pd.Series(keywords).value_counts().index[:5] # Keywords formatting as a list\n",
    "            keywordString = \" \".join(keywordExtracted) # Keywords formatting as a string\n",
    "            keywordDict = {i+1:keywordExtracted[i] for i in range(5)}\n",
    "            return keywordDict\n",
    "        except PdfReadError as pre: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {pre} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as os: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {os} happened. Please retry the task once the issue has been resolved.\"\n",
    "    else:\n",
    "        return f\"Oops! The task failed. Please retry the task once the issue has been resolved.\" # Message if program fails to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper, Pillai and Nair demonstrate that a new algorithmic model for predicting software costs and other management parameters can be improved over the Putnam SLIM model. They also demonstrate that the new model is more reliable than the previous one. The authors briefly discuss the different types of modeling used to predict \"software productivity\" and how they differ from each other. They discuss the various ways in which different models can be compared and conclude that the putnam model is most effective for predicting manpower requirements during the early stages of a project as well as for predicting future performance. In addition, they discuss several different kinds of prediction tools that can be used to estimate the speed and accuracy of a given model. An example of one of these tools is the \"routinely run-down weighted average,\" which measures the rate at which a piece of machinery moves over a set of fixed time lines. Another tool is the random noise test, which allows an observer to run a straight line through a sample of data and see how much it changes over time. A final section of the paper discusses some of the features of both the rationally estimated and the real-time versions of the mean time series.\n"
     ]
    }
   ],
   "source": [
    "# Summarize text\n",
    "text = sumArticle2(ArticleToSummarize)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'model', 2: 'data', 3: 'putnam', 4: 'manpower', 5: 'gamma'}\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords and output them in dictionary format\n",
    "keywords = pdfKE(ArticleToSummarize, 'english')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dit document tonen Pillai en Nair aan dat een nieuw algoritmisch model voor het voorspellen van de softwarekosten en andere beheerparameters verbeterd kan worden ten opzichte van het Putnam SLIM-model. Ze tonen ook aan dat het nieuwe model betrouwbaarder is dan het vorige. De auteurs bespreken kort de verschillende modellen die gebruikt worden om de \"softwareproductiviteit\" te voorspellen en hoe ze van elkaar verschillen. Ze bespreken de verschillende manieren waarop verschillende modellen kunnen worden vergeleken en concluderen dat het putnammodel het meest effectief is voor het voorspellen van de mankrachtvereisten tijdens de vroege stadia van een project en voor het voorspellen van toekomstige prestaties. Daarnaast bespreken ze verschillende soorten voorspellingsinstrumenten die kunnen worden gebruikt om de snelheid en nauwkeurigheid van een bepaald model te schatten. Een voorbeeld van een van deze instrumenten is het \"routinely run-down gewogen gemiddelde,\" dat de snelheid meet waarbij een stuk machine over een set vaste tijdlijnen beweegt. Een ander instrument is de willekeurige ruistest, die een directe lijn door een steekproef van gegevens laat lopen en hoeveel het verandert in de tijd.\n"
     ]
    }
   ],
   "source": [
    "# Summarization translation\n",
    "translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-nl\") # Model used for the translation, imported from Huggingface (https://huggingface.co/Helsinki-NLP/opus-mt-en-nl)\n",
    "translatedText = translation(text)[0]['translation_text']\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: Kan opgegeven procedure niet vinden.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11644\\2432852475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cell only works in environment that works on pip. Evaluate isn't a library that has a conda recipe.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sacrebleu\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Model generated translation evaluated with DeepL generated translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\evaluate\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m from .evaluator import (\n\u001b[0;32m     31\u001b[0m     \u001b[0mAutomaticSpeechRecognitionEvaluator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\pyarrow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0m_gc_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_gc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misenabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0m_gc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_gc_enabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0m_gc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: Kan opgegeven procedure niet vinden."
     ]
    }
   ],
   "source": [
    "# Cell only works in environment that works on pip. Evaluate isn't a library that has a conda recipe.\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\") # Model generated translation evaluated with DeepL generated translation\n",
    "\n",
    "evaluationText = \"In dit artikel worden verschillende soorten modellen besproken die worden gebruikt om de ontwikkelingstijd en -kosten van software te voorspellen. Het meest gebruikte model is het Putnam SLIM model, dat gebaseerd is op een hellings-leercurve. De auteurs zijn echter van mening dat dit model niet betrouwbaar is bij het voorspellen van zeer vroege stadia van een project. Ze gebruiken een andere benadering, het 'Putnam-model', om toekomstige prestaties te voorspellen. In dit artikel stellen ze een nieuw type model voor dat toekomstige prestaties veel nauwkeuriger voorspelt dan het vorige Putnam-model. Dit nieuwe model kan op verschillende manieren worden geÃ¯mplementeerd, waaronder: het correleren van statistieken van real-time experimenten met machine learning, het gebruik van statistische methoden zoals rinterpreter en correlatieanalyse, modellering door directe observatie en het gebruik van geavanceerde statistische technieken zoals gewogen gemiddelden. De auteurs bespreken ook verschillende benaderingen voor het voorspellen van toekomstige prestaties van een bepaald soort product of functie. Als een bedrijf bijvoorbeeld een nieuw stuk machine zou introduceren, zou het in staat moeten zijn om het snel en op een hoog niveau te produceren over een lange periode.\"\n",
    "\n",
    "translatedText = \"Het meest gebruikte model is het Putnam SLIM model, dat gebaseerd is op een hellings-learning curve. Echter, de auteurs geloven dat het niet betrouwbaar is in het voorspellen van zeer vroege stadia van een project. Ze gebruiken een andere aanpak genaamd het 'Putnam model' om toekomstige prestaties te voorspellen. In dit document suggereren ze een nieuw type model dat toekomstige prestaties veel nauwkeuriger voorspelt dan het vorige Putnam model. Dit nieuwe model kan worden toegepast op verschillende manieren, waaronder: het correleren van statistieken van real-time experimenten met machine learning, gebruik makend van statistische methoden zoals rinterpreter en correlatieanalyse, modelleren door middel van directe observatie, en gebruik makend van geavanceerde statistische technieken zoals gewogen gemiddelde. De auteurs bespreken ook verschillende benaderingen voor het voorspellen van toekomstige prestaties van een bepaald soort product of functie. Bijvoorbeeld, als een bedrijf een nieuw stuk machines zou moeten kunnen introduceren, zou het snel en op een hoog niveau van tijd moeten produceren.\"\n",
    "\n",
    "predictions = [\n",
    "    translatedText\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        evaluationText\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "### ==============================\n",
    "#{'score': 53.86425158362457,\n",
    "#'counts': [149, 112, 95, 80],\n",
    "#'totals': [173, 172, 171, 170],\n",
    "#'precisions': [86.1271676300578,\n",
    "# 65.11627906976744,\n",
    "# 55.55555555555556,\n",
    "# 47.05882352941177],\n",
    "#'bp': 0.8704644809074915,\n",
    "#'sys_len': 173,\n",
    "#'ref_len': 197}\n",
    "### ==============================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
