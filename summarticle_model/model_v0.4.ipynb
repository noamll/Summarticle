{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # only uncomment if not already downloaded\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6303f79-458a-46b0-94af-820a3a4d65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleToSummarize = \"pdf_database\\pdf1.pdf\"\n",
    "\n",
    "def pdfTE(pdfFile): # Function for text extraction from PDF.\n",
    "    with open(pdfFile, \"rb\") as file: # Read in binary to handle breakline statements better (\\n)\n",
    "        pdfReader = PdfReader(file)\n",
    "        for page in pdfReader.pages:\n",
    "            yield page.extract_text() # Use of generator as keeping the whole article in the memory results in memory error.\n",
    "            \n",
    "def articleC(pdfFile): # Function to return the article in one string\n",
    "    textCombiner = pdfTE(pdfFile) \n",
    "    textCombined = \"\"\n",
    "    for text in textCombiner: # Loop over generator object to sum text of pages into one string\n",
    "        textCombined += text\n",
    "    return textCombined\n",
    "\n",
    "def sumArticle1(pdfFile): # Function to summarize article per page\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\") # Model used from the huggingface hub (https://huggingface.co/pszemraj/led-large-book-summary)\n",
    "            for text in pdfTE(pdfFile): # Iterate over generator\n",
    "                summarizedPage = summarizer(text,max_length=100, min_length=50, do_sample=True)\n",
    "                print(summarizedPage, end=\"\\n\")\n",
    "                print()\n",
    "        except PdfReadError as pre: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {pre} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as os: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {os} happened. Please retry the task once the issue has been resolved.\"\n",
    "    else:\n",
    "        return f\"Oops! The task failed. Please retry the task once the issue has been resolved.\" # Message if program fails to execute\n",
    "\n",
    "def sumArticle2(pdfFile): # Function to summarize article as a whole\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\") # Model used from the huggingface hub (https://huggingface.co/pszemraj/led-large-book-summary)\n",
    "            articleCombined = articleC(pdfFile) # Iterate over generator\n",
    "            summarizedPage = summarizer(articleCombined,max_length=1000, min_length=200, do_sample=True)\n",
    "            return summarizedPage[0][\"summary_text\"]\n",
    "        except PdfReadError as pre: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {pre} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as os: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {os} happened. Please retry the task once the issue has been resolved.\"\n",
    "    else:\n",
    "        return f\"Oops! The task failed. Please retry the task once the issue has been resolved.\" # Message if program fails to execute\n",
    "\n",
    "def pdfKE(pdfFile, language): # Function to extract keywords from the PDF, language input needed as language needs to be part of stopwords folder.\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            articleCombined = articleC(pdfFile).lower() # Iterate over generator\n",
    "            tokens = word_tokenize(articleCombined) # Tokenize all words in the article\n",
    "            punctuations = [\"(\",\")\",\";\",\":\",\"[\",\"]\",\",\",\"!\",\"=\",\"==\",\"<\",\">\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\".\",\"//\",\"{\",\"}\",\"...\",\"``\",\"+\",\"\\'\\'\",\"-\",\"~\",\"\\\"\",\"’\",]\n",
    "            stopWords = stopwords.words(f'{language}')\n",
    "            keywords = [word for word in tokens if word not in stopWords and word not in punctuations] # Filter the words so that mostlikely keywords will be extracted\n",
    "            keywordExtracted = pd.Series(keywords).value_counts().index[:5] # Keywords formatting as a list\n",
    "            keywordString = \" \".join(keywordExtracted) # Keywords formatting as a string\n",
    "            keywordDict = {i+1:keywordExtracted[i] for i in range(5)}\n",
    "            return keywordDict\n",
    "        except PdfReadError as pre: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {pre} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as os: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {os} happened. Please retry the task once the issue has been resolved.\"\n",
    "    else:\n",
    "        return f\"Oops! The task failed. Please retry the task once the issue has been resolved.\" # Message if program fails to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper discusses several different types of models used to predict software development time and cost. The most commonly used is the Putnam model, which is based on a weighted average of various estimations made using standard methods such as logistic regression. However, the authors believe that it can be improved by using a more advanced statistical method called the \"routinely weighted mean\" which takes into account factors such as hours, days, and weeks. The authors suggest that this method can be extended to cover a wide range of industrial applications from manufacturing to space exploration to military planning to civil engineering. They claim that this new approach can predict future performance much more accurately than the previous methods. Another influential aspect of the study is the use of machine learning to predict real-time trends in the demand side of a business. For example, one of the main points in the paper is that the machine learning algorithms predicts future performance within a specific business at a fixed time frame. Machine learning has been shown to predict both the speed and the amount of work done during a given period of time.\n"
     ]
    }
   ],
   "source": [
    "# Summarize text\n",
    "text = sumArticle2(ArticleToSummarize)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'model', 2: 'data', 3: 'putnam', 4: 'manpower', 5: 'gamma'}\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords and output them in string format\n",
    "keywords = pdfKE(ArticleToSummarize, 'english')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dit document bespreekt verschillende soorten modellen die gebruikt worden om de ontwikkelingstijd en -kosten van software te voorspellen. De meest gebruikte is het Putnam-model, dat gebaseerd is op een gewogen gemiddelde van verschillende schattingen met behulp van standaardmethoden zoals logistieke regressie. Echter, de auteurs geloven dat het kan worden verbeterd door gebruik te maken van een meer geavanceerde statistische methode genaamd de \"routinely gewogen gemiddelde\" die rekening houdt met factoren zoals uren, dagen en weken. De auteurs suggereren dat deze methode kan worden uitgebreid tot een breed scala van industriële toepassingen, van productie tot ruimteverkenning tot militaire planning tot civiele techniek. Zij beweren dat deze nieuwe benadering toekomstige prestaties veel nauwkeuriger kan voorspellen dan de vorige methoden. Een ander invloedrijk aspect van de studie is het gebruik van machine learning om real-time trends in de vraagzijde van een bedrijf te voorspellen. Bijvoorbeeld, een van de belangrijkste punten in het papier is dat de machine learning algoritmes voorspelt toekomstige prestaties binnen een bepaald bedrijf op een bepaald tijdskader.\n"
     ]
    }
   ],
   "source": [
    "# Summarization translation\n",
    "translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-nl\") # Model used for the translation, imported from Huggingface (https://huggingface.co/Helsinki-NLP/opus-mt-en-nl)\n",
    "translatedText = translation(text)[0]['translation_text']\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: Kan opgegeven procedure niet vinden.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6920\\931630526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sacrebleu\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Model generated translation evaluated with DeepL generated translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mevaluation_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"In dit artikel worden verschillende soorten modellen besproken die worden gebruikt om de ontwikkelingstijd en -kosten van software te voorspellen. Het meest gebruikte model is het Putnam SLIM model, dat gebaseerd is op een hellings-leercurve. De auteurs zijn echter van mening dat dit model niet betrouwbaar is bij het voorspellen van zeer vroege stadia van een project. Ze gebruiken een andere benadering, het 'Putnam-model', om toekomstige prestaties te voorspellen. In dit artikel stellen ze een nieuw type model voor dat toekomstige prestaties veel nauwkeuriger voorspelt dan het vorige Putnam-model. Dit nieuwe model kan op verschillende manieren worden geïmplementeerd, waaronder: het correleren van statistieken van real-time experimenten met machine learning, het gebruik van statistische methoden zoals rinterpreter en correlatieanalyse, modellering door directe observatie en het gebruik van geavanceerde statistische technieken zoals gewogen gemiddelden. De auteurs bespreken ook verschillende benaderingen voor het voorspellen van toekomstige prestaties van een bepaald soort product of functie. Als een bedrijf bijvoorbeeld een nieuw stuk machine zou introduceren, zou het in staat moeten zijn om het snel en op een hoog niveau te produceren over een lange periode.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\evaluate\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m from .evaluator import (\n\u001b[0;32m     31\u001b[0m     \u001b[0mAutomaticSpeechRecognitionEvaluator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jklus\\anaconda3\\lib\\site-packages\\pyarrow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0m_gc_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_gc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misenabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0m_gc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_gc_enabled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0m_gc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: Kan opgegeven procedure niet vinden."
     ]
    }
   ],
   "source": [
    "# Cell only works in environment that works on pip. Evaluate isn't a library that has a conda recipe.\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\") # Model generated translation evaluated with DeepL generated translation\n",
    "\n",
    "evaluationText = \"In dit artikel worden verschillende soorten modellen besproken die worden gebruikt om de ontwikkelingstijd en -kosten van software te voorspellen. Het meest gebruikte model is het Putnam SLIM model, dat gebaseerd is op een hellings-leercurve. De auteurs zijn echter van mening dat dit model niet betrouwbaar is bij het voorspellen van zeer vroege stadia van een project. Ze gebruiken een andere benadering, het 'Putnam-model', om toekomstige prestaties te voorspellen. In dit artikel stellen ze een nieuw type model voor dat toekomstige prestaties veel nauwkeuriger voorspelt dan het vorige Putnam-model. Dit nieuwe model kan op verschillende manieren worden geïmplementeerd, waaronder: het correleren van statistieken van real-time experimenten met machine learning, het gebruik van statistische methoden zoals rinterpreter en correlatieanalyse, modellering door directe observatie en het gebruik van geavanceerde statistische technieken zoals gewogen gemiddelden. De auteurs bespreken ook verschillende benaderingen voor het voorspellen van toekomstige prestaties van een bepaald soort product of functie. Als een bedrijf bijvoorbeeld een nieuw stuk machine zou introduceren, zou het in staat moeten zijn om het snel en op een hoog niveau te produceren over een lange periode.\"\n",
    "\n",
    "translatedText = \"Het meest gebruikte model is het Putnam SLIM model, dat gebaseerd is op een hellings-learning curve. Echter, de auteurs geloven dat het niet betrouwbaar is in het voorspellen van zeer vroege stadia van een project. Ze gebruiken een andere aanpak genaamd het 'Putnam model' om toekomstige prestaties te voorspellen. In dit document suggereren ze een nieuw type model dat toekomstige prestaties veel nauwkeuriger voorspelt dan het vorige Putnam model. Dit nieuwe model kan worden toegepast op verschillende manieren, waaronder: het correleren van statistieken van real-time experimenten met machine learning, gebruik makend van statistische methoden zoals rinterpreter en correlatieanalyse, modelleren door middel van directe observatie, en gebruik makend van geavanceerde statistische technieken zoals gewogen gemiddelde. De auteurs bespreken ook verschillende benaderingen voor het voorspellen van toekomstige prestaties van een bepaald soort product of functie. Bijvoorbeeld, als een bedrijf een nieuw stuk machines zou moeten kunnen introduceren, zou het snel en op een hoog niveau van tijd moeten produceren.\"\n",
    "\n",
    "predictions = [\n",
    "    translatedText\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        evaluationText\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "### ==============================\n",
    "#{'score': 53.86425158362457,\n",
    "#'counts': [149, 112, 95, 80],\n",
    "#'totals': [173, 172, 171, 170],\n",
    "#'precisions': [86.1271676300578,\n",
    "# 65.11627906976744,\n",
    "# 55.55555555555556,\n",
    "# 47.05882352941177],\n",
    "#'bp': 0.8704644809074915,\n",
    "#'sys_len': 173,\n",
    "#'ref_len': 197}\n",
    "### ==============================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
