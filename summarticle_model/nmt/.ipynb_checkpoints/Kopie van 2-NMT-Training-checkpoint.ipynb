{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/ymoslem/OpenNMT-Tutorial/blob/main/2-NMT-Training.ipynb","timestamp":1696321740724}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"vSUyCs23M_H2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696329933026,"user_tz":-120,"elapsed":16565,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}},"outputId":"d152cd9e-ca0b-4b15-db49-0060a43163c9"},"source":["# Install OpenNMT-py 3.x\n","!pip3 install OpenNMT-py"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting OpenNMT-py\n","  Downloading OpenNMT_py-3.4.1-py3-none-any.whl (252 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch<2.1,>=2.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.0.1+cu118)\n","Collecting configargparse (from OpenNMT-py)\n","  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n","Collecting ctranslate2<4,>=3.2 (from OpenNMT-py)\n","  Downloading ctranslate2-3.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.13.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n","Collecting waitress (from OpenNMT-py)\n","  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n","  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n","Collecting sacrebleu (from OpenNMT-py)\n","  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n","  Downloading rapidfuzz-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n","  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.6.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.2->OpenNMT-py) (1.23.5)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.57.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.4.4)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.3.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0->OpenNMT-py) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0->OpenNMT-py) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0->OpenNMT-py) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0->OpenNMT-py) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0->OpenNMT-py) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=2.0->OpenNMT-py) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=2.0->OpenNMT-py) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=2.0->OpenNMT-py) (16.0.6)\n","Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n","  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n","Collecting portalocker (from sacrebleu->OpenNMT-py)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n","Collecting colorama (from sacrebleu->OpenNMT-py)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.10.12)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.1,>=2.0->OpenNMT-py) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=2.0->OpenNMT-py) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n","Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n","Successfully installed OpenNMT-py-3.4.1 colorama-0.4.6 configargparse-1.7 ctranslate2-3.20.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.3.1 sacrebleu-2.3.1 waitress-2.1.2\n"]}]},{"cell_type":"markdown","source":["# Prepare Your Datasets\n","Please make sure you have completed the [first exercise](https://colab.research.google.com/drive/1rsFPnAQu9-_A6e2Aw9JYK3C8mXx9djsF?usp=sharing)."],"metadata":{"id":"vhgIdJn-cLqu"}},{"cell_type":"code","metadata":{"id":"dWVOWYedzZ_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696329933026,"user_tz":-120,"elapsed":10,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}},"outputId":"0e27ac2b-f1a9-4b20-c98a-d6675a783dee"},"source":["# Open the folder where you saved your prepapred datasets from the first exercise\n","# You might need to mount your Google Drive first\n","%cd /content/drive/MyDrive/nmt/\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/nmt/'\n","/content\n","sample_data\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"UF2PU_YnNSN-","executionInfo":{"status":"error","timestamp":1696329947031,"user_tz":-120,"elapsed":14010,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}},"outputId":"6895bc44-4803-4374-f3b2-1cefa110f940"},"execution_count":3,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"markdown","metadata":{"id":"MPlmhd426B7l"},"source":["# Create the Training Configuration File\n","\n","The following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values:\n","* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n","* `valid_steps` - 10000 can be good if the value `train_steps` is big enough.\n","* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n","\n","Refer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)."]},{"cell_type":"code","metadata":{"id":"qbW7Xek6UDlY","executionInfo":{"status":"aborted","timestamp":1696329947031,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Create the YAML configuration file\n","# On a regular machine, you can create it manually or with nano\n","# Note here we are using some smaller values because the dataset is small\n","# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n","\n","config = '''# config.yaml\n","\n","\n","## Where the samples will be written\n","save_data: run\n","\n","# Training files\n","data:\n","    corpus_1:\n","        path_src: WikiMatrix.en-nl.en-filtered.en.subword.train\n","        path_tgt: WikiMatrix.en-nl.nl-filtered.nl.subword.train\n","        transforms: [filtertoolong]\n","    valid:\n","        path_src: WikiMatrix.en-nl.en-filtered.en.subword.dev\n","        path_tgt: WikiMatrix.en-nl.nl-filtered.nl.subword.dev\n","        transforms: [filtertoolong]\n","\n","# Vocabulary files, generated by onmt_build_vocab\n","src_vocab: run/source.vocab\n","tgt_vocab: run/target.vocab\n","\n","# Vocabulary size - should be the same as in sentence piece\n","src_vocab_size: 50000\n","tgt_vocab_size: 50000\n","\n","# Filter out source/target longer than n if [filtertoolong] enabled\n","src_seq_length: 150\n","src_seq_length: 150\n","\n","# Tokenization options\n","src_subword_model: source.model\n","tgt_subword_model: target.model\n","\n","# Where to save the log file and the output models/checkpoints\n","log_file: train.log\n","save_model: models/model.fren\n","\n","# Stop training if it does not imporve after n validations\n","early_stopping: 4\n","\n","# Default: 5000 - Save a model checkpoint for each n\n","save_checkpoint_steps: 1000\n","\n","# To save space, limit checkpoints to last n\n","# keep_checkpoint: 3\n","\n","seed: 3435\n","\n","# Default: 100000 - Train the model to max n steps\n","# Increase to 200000 or more for large datasets\n","# For fine-tuning, add up the required steps to the original steps\n","train_steps: 3000\n","\n","# Default: 10000 - Run validation after n steps\n","valid_steps: 1000\n","\n","# Default: 4000 - for large datasets, try up to 8000\n","warmup_steps: 1000\n","report_every: 100\n","\n","# Number of GPUs, and IDs of GPUs\n","world_size: 1\n","gpu_ranks: [0]\n","\n","# Batching\n","bucket_size: 262144\n","num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n","batch_type: \"tokens\"\n","batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n","valid_batch_size: 2048\n","max_generator_batches: 2\n","accum_count: [4]\n","accum_steps: [0]\n","\n","# Optimization\n","model_dtype: \"fp16\"\n","optim: \"adam\"\n","learning_rate: 2\n","# warmup_steps: 8000\n","decay_method: \"noam\"\n","adam_beta2: 0.998\n","max_grad_norm: 0\n","label_smoothing: 0.1\n","param_init: 0\n","param_init_glorot: true\n","normalization: \"tokens\"\n","\n","# Model\n","encoder_type: transformer\n","decoder_type: transformer\n","position_encoding: true\n","enc_layers: 6\n","dec_layers: 6\n","heads: 8\n","hidden_size: 512\n","word_vec_size: 512\n","transformer_ff: 2048\n","dropout_steps: [0]\n","dropout: [0.1]\n","attention_dropout: [0.1]\n","'''\n","\n","with open(\"config.yaml\", \"w+\") as config_yaml:\n","  config_yaml.write(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsL4zycvLMUx","executionInfo":{"status":"aborted","timestamp":1696329947031,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# [Optional] Check the content of the configuration file\n","!cat config.yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0bcqYkEXhRY"},"source":["# Build Vocabulary\n","\n","For large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, a specific set of vocabulary is extracted from the training dataset, usually betweeen 32k and 100k words. This is the main purpose of the vocabulary building step."]},{"cell_type":"code","metadata":{"id":"AuwltKp_VhnQ","executionInfo":{"status":"aborted","timestamp":1696329947031,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Find the number of CPUs/cores on the machine\n","!nproc --all"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2GV1PgyUsJr","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":8,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Build Vocabulary\n","\n","# -config: path to your config.yaml file\n","# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n","# -num_threads: change it to match the number of CPUs to run it faster\n","\n","!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncWyNtxiO_Ov"},"source":["From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\".\n"]},{"cell_type":"code","metadata":{"id":"TMMPeS-pSV8I","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Check if the GPU is active\n","!nvidia-smi -L"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3rVQhd4NXNG","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Check if the GPU is visable to PyTorch\n","\n","import torch\n","\n","print(torch.cuda.is_available())\n","print(torch.cuda.get_device_name(0))\n","\n","gpu_memory = torch.cuda.mem_get_info(0)\n","print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8aCxETSnXcL-"},"source":["# Training\n","\n","Now, start training your NMT model! ğŸ‰ ğŸ‰ ğŸ‰"]},{"cell_type":"code","source":["!rm -rf drive/MyDrive/nmt/models/"],"metadata":{"id":"HZd1o1kIb6Nv","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prJCKA2CP-dl","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Train the NMT model\n","!onmt_train -config config.yaml"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For error debugging try:\n","# !dmesg -T"],"metadata":{"id":"XUYAvE8ffK2k","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eShpS01j-Jcp"},"source":["# Translation\n","\n","Translation Options:\n","* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n","* `-src` - the subworded test dataset, source file\n","* `-output` - give any file name to the new translation output file\n","* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n","* `-min_length` - [optional] to avoid empty translations\n","* `-verbose` - [optional] if you want to print translations\n","\n","Refer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details."]},{"cell_type":"code","metadata":{"id":"MbQEGTj4TybH","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Translate the \"subworded\" source file of the test dataset\n","# Change the model name, if needed.\n","!onmt_translate -model models/model.fren_step_3000.pt -src WikiMatrix.en-nl.en.subword.test -output WikiMatrix.nl.translated -gpu 0 -min_length 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHYihrgfIrIO","executionInfo":{"status":"aborted","timestamp":1696329947032,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Check the first 5 lines of the translation file\n","!head -n 5 WikiMatrix.nl.translated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRsJm6UET2C_","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# If needed install/update sentencepiece\n","!pip3 install --upgrade -q sentencepiece\n","\n","# Desubword the translation file\n","!python3 MT-Preparation/subwording/3-desubword.py target.model WikiMatrix.nl.translated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ai4RhhGaKBp1","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Check the first 5 lines of the desubworded translation file\n","!head -n 5 WikiMatrix.nl.translated.desubword"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOUWB4r3OFOV","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Desubword the target file (reference) of the test dataset\n","# Note: You might as well have split files *before* subwording during dataset preperation,\n","# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n","!python3 MT-Preparation/subwording/3-desubword.py target.model WikiMatrix.en-nl.nl.subword.test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jULN0MwOFeH","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Check the first 5 lines of the desubworded reference\n","!head -n 5 WikiMatrix.en-nl.nl.subword.test.desubword"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHMumxqvLDDc"},"source":["# MT Evaluation\n","\n","There are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.\n","\n","Here we are using BLEU. Files must be detokenized/desubworded beforehand."]},{"cell_type":"code","metadata":{"id":"w-9XGYnaJ-Nj","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Download the BLEU script\n","!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYDG0x0KLk_O","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Install sacrebleu\n","!pip3 install sacrebleu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3V3tZphTzK9","executionInfo":{"status":"aborted","timestamp":1696329947033,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jeroen Kluskens","userId":"03009847662037463989"}}},"source":["# Evaluate the translation (without subwording)\n","!python3 compute-bleu.py WikiMatrix.en-nl.nl.subword.test.desubword WikiMatrix.nl.translated.desubword"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBi1PhRv4bX9"},"source":["# More Features and Directions to Explore\n","\n","Experiment with the following ideas:\n","* Icrease `train_steps` and see to what extent new checkpoints provide better translation, in terms of both BLEU and your human evaluation.\n","\n","* Check other MT Evaluation mentrics other than BLEU such as [TER](https://github.com/mjpost/sacrebleu#ter), [WER](https://blog.machinetranslation.io/compute-wer-score/), [METEOR](https://blog.machinetranslation.io/compute-bleu-score/#meteor), [COMET](https://github.com/Unbabel/COMET), and [BERTScore](https://github.com/Tiiiger/bert_score). What are the conceptual differences between them? Is there special cases for using a specific metric?\n","\n","* Continue training from the last model checkpoint using the `-train_from` option, only if the training stopped and you want to continue it. In this case, `train_steps` in the config file should be larger than the steps of the last checkpoint you train from.\n","```\n","!onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt\n","```\n","\n","* **Ensemble Decoding:** During translation, instead of adding one model/checkpoint to the `-model` argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?\n","\n","* **Averaging Models:** Try to average multiple models into one model using the [average_models.py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/bin/average_models.py) script, and see how this affects translation quality.\n","```\n","python3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt\n","```\n","* **Release the model:** Try this command and see how it reduce the model size.\n","```\n","onmt_release_model --model \"model.pt\" --output \"model_released.pt\n","```\n","* **Use CTranslate2:** For efficient translation, consider using [CTranslate2](https://github.com/OpenNMT/CTranslate2), a fast inference engine. Check out an [example](https://gist.github.com/ymoslem/60e1d1dc44fe006f67e130b6ad703c4b).\n","\n","* **Work on low-resource languages:** Find out more details about [how to train NMT models for low-resource languages](https://blog.machinetranslation.io/low-resource-nmt/).\n","\n","* **Train a multilingual model:** Find out helpful notes about [training multilingual models](https://blog.machinetranslation.io/multilingual-nmt).\n","\n","* **Publish a demo:** Show off your work through a [simple demo with CTranslate2 and Streamlit](https://blog.machinetranslation.io/nmt-web-interface/).\n"]}]}