{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1089a-7c20-4cec-a9b4-567a9f3cdff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3eb57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pyodbc\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee2fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # only uncomment if not already downloaded\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6303f79-458a-46b0-94af-820a3a4d65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translation & Summarization functions\n",
    "ArticleToSummarize = \"pdf_database\\pdf3.pdf\"\n",
    "\n",
    "def pdfTE(pdfFile, version=1, start=0, end=0): # Function for text extraction from PDF. # Version states which version to use, with arguments(start, end) being the starting page and ending page.\n",
    "    if version==1:\n",
    "        with open(pdfFile, \"rb\") as file: # Read in binary to handle breakline statements better (\\n)\n",
    "            pdfReader = PdfReader(file)\n",
    "            for page in pdfReader.pages:\n",
    "                yield page.extract_text() # Use of generator as keeping the whole article in the memory results in memory error.\n",
    "    else:\n",
    "        with open(pdfFile, \"rb\") as file: # Read in binary to handle breakline statements better (\\n)\n",
    "            pdfReader = PdfReader(file)\n",
    "            if end==0:\n",
    "                end=len(pdfReader.pages)-1\n",
    "            for num in range(start,end): # Iterator over page (num) \n",
    "                yield pdfReader.pages[num].extract_text() # Text extraction of page (num)\n",
    "            \n",
    "def articleC(pdfFile, version=1,start=0,end=0): # Function to return the article in one string # Version and start, end arguments necessary for pdfTE integration.\n",
    "    if version==1:\n",
    "        textCombiner = pdfTE(pdfFile)\n",
    "    else:\n",
    "        textCombiner = pdfTE(pdfFile,2,start,end) # Choosing for version 2 if sumArticle2 failed on version 1\n",
    "    textCombined = \"\"\n",
    "    for text in textCombiner: # Loop over generator object to sum text of pages into one string\n",
    "        textCombined += text\n",
    "    if textCombined == \"\":\n",
    "        raise Exception(\"Oops! The task failed as the PDF file is empty.\") # Exception that handles empty files\n",
    "    elif len(textCombined) < 10:\n",
    "        raise Exception(\"Oops! The task failed as the PDF file has too little characters.\") # Exception that handles files with too little characters for a summary\n",
    "    else:\n",
    "        return textCombined\n",
    "\n",
    "def sumArticle2(pdfFile): # Function to summarize article as a whole\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\") # Model used from the huggingface hub (https://huggingface.co/pszemraj/led-large-book-summary)\n",
    "            articleCombined = articleC(pdfFile) # Iterate over text combiner function\n",
    "            lengthArticle = len(articleCombined)\n",
    "            summarizedPage = summarizer(articleCombined,max_length=1000, min_length=200, do_sample=True) # Max/min length for length of summarization in characters.\n",
    "            return summarizedPage[0][\"summary_text\"] # Return summary as string\n",
    "        except PdfReadError as PRE: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {PRE} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as OS: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {OS} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except (RuntimeError, IndexError) as RIE: # Exception check if pdf fails to be summarized due to it having too many tokens (> 16384)\n",
    "            print(f\"Oops! {RIE}, The file was too big to configure.\")\n",
    "            try:\n",
    "                print(\"Warning, this summary may take a while to produce.\\n\") # Give feedback to the user that because of the large pdf, the models computation will be larger\n",
    "                start = 0\n",
    "                summary = \"\" # Empty string to merge all the separate summaries in\n",
    "                length = len(PdfReader(ArticleToSummarize).pages) # Amount of pages in article\n",
    "                amountPages = (length//(lengthArticle//65536+1))+1 # Optimalization to determine best end to achieve least amount of computation (65536 amount of chars model can handle)\n",
    "                end = amountPages\n",
    "                lastRun = False\n",
    "                while True: # Loop till all the pages have been summarized\n",
    "                    articleCombined = articleC(pdfFile,2,start,end)\n",
    "                    summarizedPage = summarizer(articleCombined,max_length=1000, min_length=200, do_sample=False) # Max/min length for length of summarization  per amount of pages in characters.\n",
    "                    summary += f\"{summarizedPage[0]['summary_text']} \"\n",
    "                    start += amountPages # Calculations to determine next start/end point of iteration.\n",
    "                    end += amountPages\n",
    "                    if lastRun or start>=length: # Checker if program needs to stop executing\n",
    "                        break\n",
    "                    if end>length or end==length: # Once summarizer reaches last page in summarization, it tells program to stop after one last run\n",
    "                        end=length-1\n",
    "                        lastRun = True\n",
    "    \n",
    "                summaryF = summarizer(summary,max_length=2000, min_length=200, do_sample=False)        \n",
    "                return summaryF[0]['summary_text']\n",
    "            except Exception as E: # Exception that handles all different errors and asks user to feedback the error to the devs to enhance the model.\n",
    "                return f\"Oops! An unexpected error occured, {E}. Please report the error to the team.\"\n",
    "    else:\n",
    "        raise Exception(\"The file format is not a valid pdf.\")\n",
    "\n",
    "def pdfKE(pdfFile, language='english'): # Function to extract keywords from the PDF, language input needed as language needs to be part of stopwords folder. Standard keyword is english\n",
    "    if pdfFile.endswith(\".pdf\"): # First check if file extension is \".pdf\" format.\n",
    "        try:\n",
    "            articleCombined = articleC(pdfFile).lower() # Iterate over generator\n",
    "            tokens = word_tokenize(articleCombined) # Tokenize all words in the article\n",
    "            punctuations = [\"(\",\")\",\";\",\":\",\"[\",\"]\",\",\",\"!\",\"=\",\"==\",\"<\",\">\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\".\",\"//\",\"{\",\"}\",\"...\",\"``\",\"+\",\"\\'\\'\",\"-\",\"~\",\"\\\"\",\"â€™\",]\n",
    "            stopWords = stopwords.words(f'{language}')\n",
    "            keywords = [word for word in tokens if word not in stopWords and word not in punctuations] # Filter the words so that mostlikely keywords will be extracted\n",
    "            keywordExtracted = pd.Series(keywords).value_counts().index[:5] # Keywords formatting as a list\n",
    "            keywordDict = {i+1:keywordExtracted[i] for i in range(5)}\n",
    "            return keywordDict\n",
    "        except PdfReadError as PRE: # Second check if file extension is \".pdf\" format when first check fails.\n",
    "            return f\"Oops! a PDF Read Error {PRE} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except OSError as OS: # Exception handling of OS errors, as PdfReadError doesn't catch all file extension errors.\n",
    "            return f\"Oops! an OS Error {OS} happened. Please retry the task once the issue has been resolved.\"\n",
    "        except Exception as E: # Exception that handles all different errors and asks user to feedback the error to the devs to enhance the model.\n",
    "            return f\"Oops! An unexpected error occured, {E}. Please report the error to the team.\"\n",
    "    else:\n",
    "        raise Exception(\"The file format is not a valid pdf.\")\n",
    "\n",
    "def translArticle(textToTranslate):\n",
    "    try:\n",
    "        translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-nl\") # Model used for the translation, imported from Huggingface (https://huggingface.co/Helsinki-NLP/opus-mt-en-nl)\n",
    "        translatedText = translation(textToTranslate)[0]['translation_text']\n",
    "        return translatedText\n",
    "    except Exception as E: # Exception that handles all different errors and asks user to feedback the error to the devs to enhance the model.\n",
    "        return f\"Oops! An unexpected error occured, {E}. Please report the error to the team.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e180d-3754-47f0-8bf8-474f55834f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database functions\n",
    "import pyodbc\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Replace 'your_connection_string' with your actual connection string\n",
    "connection_string = r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=Summarticle_database.accdb;'\n",
    "\n",
    "def save_paper(json_data):\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    paper_title = json_data.get(\"paper\", {}).get(\"title\")\n",
    "    \n",
    "    # Check if the paper title is not already in the Paper table\n",
    "    select_query = \"SELECT * FROM papers WHERE title = ?\"\n",
    "    cursor.execute(select_query, (paper_title,))\n",
    "    \n",
    "    if not cursor.fetchone():\n",
    "        # Paper title is not in the Paper table, save the paper\n",
    "        insert_query = \"INSERT INTO papers (title, authors, DOI, keywords) VALUES (?, ?, ?, ?)\"\n",
    "        cursor.execute(insert_query, (\n",
    "            paper_title,\n",
    "            json_data.get(\"paper\", {}).get(\"authors\"),\n",
    "            json_data.get(\"paper\", {}).get(\"DOI\"),\n",
    "            ', '.join(json_data.get(\"paper\", {}).get(\"keywords\", []))\n",
    "        ))\n",
    "        \n",
    "        connection.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def save_summary(json_data):\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    paper_title = json_data.get(\"summary\", {}).get(\"title\")\n",
    "    \n",
    "    # Check if the paper title is not already in the Summary table\n",
    "    select_query = \"SELECT * FROM summary WHERE title = ?\"\n",
    "    cursor.execute(select_query, (paper_title,))\n",
    "    \n",
    "    if not cursor.fetchone():\n",
    "        # Paper title is not in the Summary table, save the summary\n",
    "        insert_query = \"INSERT INTO summary (title, summary_en, rating_en) VALUES (?, ?, ?)\"\n",
    "        cursor.execute(insert_query, (\n",
    "            paper_title,\n",
    "            json_data.get(\"summary\", {}).get(\"summary_en\"),\n",
    "            json_data.get(\"summary\", {}).get(\"rating_en\", 0)\n",
    "        ))\n",
    "        \n",
    "        connection.commit()\n",
    "    else:\n",
    "        # Paper title is already in the Summary table, save summary in a new row\n",
    "        insert_query = \"INSERT INTO summary (title, summary_en, rating_en) VALUES (?, ?, ?)\"\n",
    "        cursor.execute(insert_query, (\n",
    "            paper_title,\n",
    "            json_data.get(\"summary\", {}).get(\"summary_en\"),\n",
    "            json_data.get(\"summary\", {}).get(\"rating_en\", 0)\n",
    "        ))\n",
    "\n",
    "        connection.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def read_summary(json_data):\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    paper_title = json_data.get(\"title\")\n",
    "    language = json_data.get(\"language\", \"en\")\n",
    "\n",
    "    # Check whether JSON GET request is from summary_en or summary_nl\n",
    "    if language == \"en\":\n",
    "        select_query = \"SELECT summary_en, rating_en FROM summary WHERE title = ? AND summary_en IS NOT NULL\"\n",
    "    elif language == \"nl\":\n",
    "        select_query = \"SELECT summary_nl, rating_nl FROM summary WHERE title = ? AND summary_nl IS NOT NULL\"\n",
    "    else:\n",
    "        return None  # Invalid language\n",
    "    \n",
    "    cursor.execute(select_query, (paper_title,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result:\n",
    "        # If there is a summary in the requested language, return it based on the rating\n",
    "        summary, rating = result\n",
    "        return summary if random.random() < rating / 10.0 else None  # Weighted random based on rating\n",
    "    else:\n",
    "        return None  # Create a new summary (you already have code for this)\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def read_keyword(text):\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    keyword = text\n",
    "\n",
    "    # Get keyword from JSON request\n",
    "    select_query = \"SELECT title FROM papers WHERE keywords LIKE ?\"\n",
    "    cursor.execute(select_query, ('%' + keyword + '%',))\n",
    "    result = cursor.fetchall()\n",
    "\n",
    "    if result:\n",
    "        # If there are papers with the specified keyword, return the titles\n",
    "        return [row[0] for row in result]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def update_rating(json_data):\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    paper_title = json_data.get(\"title\")\n",
    "    rating_type = json_data.get(\"rating_type\")\n",
    "\n",
    "    # If JSON-rating == thumbs-up, corresponding rating in table += 1\n",
    "    # If JSON-rating == thumbs-down, corresponding rating in table -= 1\n",
    "    if rating_type == \"thumbs-up\":\n",
    "        update_query = \"UPDATE summary SET rating_en = CASE WHEN rating_en + 1 > 10 THEN 10 ELSE rating_en + 1 END WHERE title = ?\"\n",
    "    elif rating_type == \"thumbs-down\":\n",
    "        update_query = \"UPDATE summary SET rating_en = CASE WHEN rating_en - 1 < 1 THEN 1 ELSE rating_en - 1 END WHERE title = ?\"\n",
    "    else:\n",
    "        return None  # Invalid rating type\n",
    "\n",
    "    cursor.execute(update_query, (paper_title,))\n",
    "    connection.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "def delete_summary():\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # If summary_rating is lower than 3, delete from Summary table\n",
    "    delete_query = \"DELETE FROM summary WHERE rating_en < 3\"\n",
    "    cursor.execute(delete_query)\n",
    "    connection.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fd5cd-8a25-4862-9f4c-00804273d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicate the summary to the orchestrator with a POST request\n",
    "orchestrator_url = \"http://ORCHESTRATOR/summary\"\n",
    "request_data = {\"paper_id\": orch_data.get(\"paper_id\"), \"summary\": text}\n",
    "response = requests.post(orchestrator_url, json=request_data)\n",
    "\n",
    "# Check the response status\n",
    "if response.status_code == 200:\n",
    "    print(\"Summary successfully communicated to the orchestrator.\")\n",
    "else:\n",
    "    print(f\"Error communicating summary to the orchestrator. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b0ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (28510 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops! index out of range in self, The file was too big to configure.\n",
      "Warning, this summary may take a while to produce.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 2000, but your input_length is only 436. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=218)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley is an autonomous, machine-learning robot designed to explore unre-heared, off-road terrain. It uses state-of-the-art artificial intelligence to predict future obstacles and make decisions quickly. The distinguishing feature of this study is the use of multiple machine learning algorithms to develop a prediction engine that outperforms previous estimates by more than a factor of ten. In both the 2004 and 2005 races, Stanley outperforms all other vehicles in terms of speed and accuracy. For the 2005 race, the safety of the vehicle poses a problem, so the team uses a combination of traditional radar, chemical/metabolic acid sensors, and several different types of sensing. These sensing devices are coupled with a \"learning algorithm\" that uses statistical methods such as averaging and batching. The goal of this approach is to predict when the vehicle will be closest to the object it is trying to reach. Stanley proves to be very accurate at times, but his overall accuracy is rather low. Using a velocity controller that constantly estimates terrain slope and ruggedness to predict the appropriate speed, Stan-ley's path planner determines the most appropriate \"velocity\" for each stage of the race. The final product is a simulation of the winning robot, Stanley; there are some hiccups, such as stalling of the laser data stream due to an incorrect time stamp; but overall, the results are very promising.\n"
     ]
    }
   ],
   "source": [
    "# IF GET REQUEST IS FOR ENGLISH SUMMARY, USE THE FOLLOWING CODE\n",
    "\n",
    "#If summary in database --> get that\n",
    "summary = read_summary(orch_data)\n",
    "if summary != None:\n",
    "    print summary\n",
    "\n",
    "else: #If there is no summary for this article\n",
    "    text = sumArticle2(ArticleToSummarize) #Summarize it using the AI model\n",
    "    print(text) #return the summary\n",
    "    save_paper(orch_data) #Save the paper\n",
    "    save_summary(orch_data) #Save the summary\n",
    "\n",
    "if orch_data.get(\"rating_type\") != None:\n",
    "    update_rating(orch_data) #update the rating, if there is some in the JSON\n",
    "    delete_summary() #delete summaries with low ratings\n",
    "    \n",
    "#Communicate the summary to the orchestrator with a POST request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e927510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'stanley', 2: 'vehicle', 3: 'figure', 4: '/h20850', 5: 'online'}\n"
     ]
    }
   ],
   "source": [
    "# IF GET REQUEST IS FOR RELATED ARTICLES, USE THE FOLLOWING CODE\n",
    "\n",
    "# Extract keywords and output them in dictionary format\n",
    "keywords = pdfKE(ArticleToSummarize)\n",
    "print(keywords)\n",
    "\n",
    "#Check if keywords exist in \n",
    "related = read_keyword(keywords)\n",
    "if related != None:\n",
    "    print(related)\n",
    "    \n",
    "if orch_data.get(\"rating_type\") != None:\n",
    "    update_rating(orch_data) #update the rating, if there is some in the JSON\n",
    "    delete_summary() #delete summaries with low ratings\n",
    "    \n",
    "#Communicate the summary to the orchestrator with a POST request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8e210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dit document beschrijft de robot Stanley, die de 2005 DARPA Grand Challenge won. Stanley is een autonome, machine-learning robot ontworpen om onherhorende, off-road terrein te verkennen. Het maakt gebruik van state-of-the-art kunstmatige intelligentie om toekomstige obstakels te voorspellen en snel beslissingen te nemen. Het onderscheidende kenmerk van deze studie is het gebruik van meerdere machine learning algoritmen om een voorspelling motor te ontwikkelen die eerder schattingen overtreft met meer dan een factor tien. In zowel de 2004 en 2005 races, Stanley overtreft alle andere voertuigen in termen van snelheid en nauwkeurigheid. Voor de 2005 race, de veiligheid van het voertuig vormt een probleem, dus het team maakt gebruik van een combinatie van traditionele radar, chemische / metabolische zure sensoren, en verschillende soorten sensors. Deze sensoren zijn gekoppeld aan een \"learning algoritme\" dat gebruik maakt van statistische methoden zoals gemiddelde en batching. Het doel van deze aanpak is om te voorspellen wanneer het voertuig het dichtst bij het object is. Stanley blijkt zeer nauwkeurig te zijn.\n"
     ]
    }
   ],
   "source": [
    "# IF GET REQUEST IS FOR DUTCH SUMMARY, USE THE FOLLOWING CODE\n",
    "\n",
    "#If there is a Dutch summary in the database --> get that\n",
    "summary = read_summary(orch_data)\n",
    "if summary != None:\n",
    "    print summary\n",
    "else: #Summarize and translate\n",
    "    text = sumArticle2(ArticleToSummarize) #Summarize it using the AI model\n",
    "    translation = translArticle(text)\n",
    "    print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
